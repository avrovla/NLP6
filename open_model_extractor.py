from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import json
import re


class OpenModelExtractor:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ {model_name}...")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto"
            )

            # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map="auto"
            )

            print(f"‚úÖ {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {e}")
            self._load_fallback_model()

    def _load_fallback_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        self.generator = pipeline(
            "text-generation",
            model="microsoft/DialoGPT-small",
            device_map="auto"
        )
        print("‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def extract_with_forced_json(self, text):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å –∂–µ—Å—Ç–∫–∏–º –ø—Ä–æ–º–ø—Ç–æ–º"""

        prompt = f"""
### SYSTEM: 
–¢—ã - API. –¢—ã –ø—Ä–∏–Ω–∏–º–∞–µ—à—å —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—à—å JSON. 
–¢—ã –ù–ï –¥–æ–±–∞–≤–ª—è–µ—à—å –Ω–∏–∫–∞–∫–∏—Ö –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤ –∫—Ä–æ–º–µ JSON.

### INPUT:
{text}

### OUTPUT FORMAT:
{"{"}
  "–ò–ù–ù": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π –∏–Ω–Ω –∏–ª–∏ null",
  "–§–ò–û": "–Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Ñ–∏–æ –∏–ª–∏ null"
{"}"}

### RESPONSE (ONLY JSON):
"""

        try:
            response = self.generator(
                prompt,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=False,
                num_return_sequences=1,
                repetition_penalty=2.0,
                pad_token_id=self.tokenizer.eos_token_id
            )[0]['generated_text']

            print("üì® –°–´–†–û–ô –û–¢–í–ï–¢:")
            print(response)
            print("-" * 50)

            return self._bruteforce_json_parse(response, text)

        except Exception as e:
            return {"error": str(e), "text": text}

    def _bruteforce_json_parse(self, response, original_text):
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON"""

        # –ú–µ—Ç–æ–¥ 1: –ò—â–µ–º JSON –º–µ–∂–¥—É —Ñ–∏–≥—É—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏
        json_match = re.search(r'\{[^{}]*\}', response)
        if json_match:
            try:
                data = json.loads(json_match.group())
                if "–ò–ù–ù" in data or "–§–ò–û" in data:
                    return self._validate_data(data, original_text)
            except:
                pass

        # –ú–µ—Ç–æ–¥ 2: –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –∑–Ω–∞—á–µ–Ω–∏—è
        inn = self._extract_by_pattern(response, r'"?–ò–ù–ù"?\s*[=:]\s*"([^"]*)"')
        fio = self._extract_by_pattern(response, r'"?–§–ò–û"?\s*[=:]\s*"([^"]*)"')

        if inn or fio:
            return {
                "–ò–ù–ù": self._clean_inn(inn),
                "–§–ò–û": self._clean_fio(fio),
                "method": "pattern_matching",
                "original_text": original_text
            }

        # –ú–µ—Ç–æ–¥ 3: –ò—â–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        return self._extract_from_original(original_text)

    def _extract_by_pattern(self, text, pattern):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        match = re.search(pattern, text, re.IGNORECASE)
        return match.group(1) if match else None

    def _clean_inn(self, inn):
        """–û—á–∏—â–∞–µ—Ç –ò–ù–ù"""
        if not inn:
            return None
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        inn_clean = re.sub(r'\D', '', str(inn))
        return inn_clean if len(inn_clean) in [10, 12] else None

    def _clean_fio(self, fio):
        """–û—á–∏—â–∞–µ—Ç –§–ò–û"""
        if not fio:
            return None
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        fio_clean = re.sub(r'[^–∞-—è–ê-–Ø—ë–Å\s]', '', str(fio)).strip()
        return fio_clean if fio_clean else None

    def _extract_from_original(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –ò–ù–ù
        inn_match = re.search(r'\b(\d{10,12})\b', text)
        inn = inn_match.group(1) if inn_match else None

        # –ò—â–µ–º –§–ò–û (2-3 —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏)
        fio_match = re.search(r'([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){1,2})', text)
        fio = fio_match.group(1) if fio_match else None

        return {
            "–ò–ù–ù": inn,
            "–§–ò–û": fio,
            "method": "direct_extraction",
            "original_text": text
        }

    def _validate_data(self, data, original_text):
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        validated = data.copy()

        if "–ò–ù–ù" in validated:
            validated["–ò–ù–ù"] = self._clean_inn(validated["–ò–ù–ù"])

        if "–§–ò–û" in validated:
            validated["–§–ò–û"] = self._clean_fio(validated["–§–ò–û"])

        validated["method"] = "json_parsing"
        validated["original_text"] = original_text

        return validated


def test_open_models():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏"""

    models_to_test = [
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-small",
        "gpt2",  # –°–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è, –Ω–æ –Ω–∞–¥–µ–∂–Ω–∞—è
    ]

    test_texts = [
        "–ê–∫–∫—Ä n 123, –ò–Ω–Ω 4353229845, –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á",
        "–ö–ª–∏–µ–Ω—Ç: –ü–µ—Ç—Ä–æ–≤ –ê–ª–µ–∫—Å–µ–π –°–µ—Ä–≥–µ–µ–≤–∏—á, –ò–ù–ù 123456789012",
        "–§–ò–û: –°–∏–¥–æ—Ä–æ–≤–∞ –ú–∞—Ä–∏—è, –∏–Ω–Ω 9876543210",
    ]

    for model_name in models_to_test:
        print(f"\n{'üéØ' * 20} –ú–û–î–ï–õ–¨: {model_name} {'üéØ' * 20}")

        try:
            extractor = OpenModelExtractor(model_name)

            for i, text in enumerate(test_texts, 1):
                print(f"\nüìù –¢–ï–°–¢ {i}: {text}")
                result = extractor.extract_with_forced_json(text)
                print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢:")
                print(json.dumps(result, ensure_ascii=False, indent=2))

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å {model_name}: {e}")
            continue


if __name__ == "__main__":
    test_open_models()