from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


class GemmaChat:
    def __init__(self):
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é Gemma...")
        try:
            # –ü—Ä–æ–±—É–µ–º –±–µ–∑ —Ç–æ–∫–µ–Ω–∞
            self.tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b-it")
            self.model = AutoModelForCausalLM.from_pretrained(
                "google/gemma-2-2b-it",
                torch_dtype=torch.float16,
                device_map="auto"
            )
            print("‚úÖ Gemma –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ —Ç–æ–∫–µ–Ω–∞!")
            self.ready = True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            print("üìù –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ. –ù—É–∂–µ–Ω —Ç–æ–∫–µ–Ω –¥–ª—è –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.")
            self.ready = False

    def chat(self, message):
        if not self.ready:
            return "‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"

        prompt = f"""<start_of_turn>user
{message}<end_of_turn>
<start_of_turn>model
"""

        inputs = self.tokenizer(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.split("<start_of_turn>model")[-1].strip()


# üé™ –¢–µ—Å—Ç
if __name__ == "__main__":
    bot = GemmaChat()

    if bot.ready:
        print("\nü§ñ Gemma –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—â–µ–Ω–∏—é!")

        test_messages = [
            "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
            "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ",
            "–ß—Ç–æ —Ç—ã —É–º–µ–µ—à—å?"
        ]

        for msg in test_messages:
            print(f"\nüë§ –í—ã: {msg}")
            response = bot.chat(msg)
            print(f"ü§ñ Gemma: {response}")