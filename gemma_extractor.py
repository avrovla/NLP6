from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import json
import re


class GemmaExtractor:
    def __init__(self):
        print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Gemma-2-2b-it...")

        model_name = "google/gemma-2-2b-it"

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device_map="auto"
        )

        print("‚úÖ Gemma –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def extract_inn_and_name(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é Gemma"""

        prompt = f"""<start_of_turn>user
–ò–∑–≤–ª–µ–∫–∏ –ò–ù–ù –∏ –§–ò–û –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–¢–µ–∫—Å—Ç: "{text}"

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
{{
  "–ò–ù–ù": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π –∏–Ω–Ω –∏–ª–∏ null",
  "–§–ò–û": "–Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Ñ–∏–æ –∏–ª–∏ null"
}}<end_of_turn>
<start_of_turn>model
"""

        try:
            response = self.generator(
                prompt,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=False,
                num_return_sequences=1,
                pad_token_id=self.tokenizer.eos_token_id
            )[0]['generated_text']

            print("üì® –°–´–†–û–ô –û–¢–í–ï–¢ GEMMA:")
            print(response)
            print("-" * 50)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            model_response = response.split("<start_of_turn>model")[-1].strip()

            return self._parse_gemma_response(model_response, text)

        except Exception as e:
            return {"error": str(e), "text": text}

    def extract_with_chat_template(self, text):
        """–ò—Å–ø–æ–ª—å–∑—É–µ–º —á–∞—Ç-—Ç–µ–º–ø–ª–µ–π—Ç Gemma"""

        messages = [
            {"role": "user", "content": f"""–ò–∑–≤–ª–µ–∫–∏ –ò–ù–ù –∏ –§–ò–û –∏–∑ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON:

–¢–µ–∫—Å—Ç: {text}

–§–æ—Ä–º–∞—Ç:
{{
  "–ò–ù–ù": "–Ω–∞–π–¥–µ–Ω–Ω—ã–π –∏–Ω–Ω –∏–ª–∏ null", 
  "–§–ò–û": "–Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Ñ–∏–æ –∏–ª–∏ null"
}}"""}
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer.encode(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        model_part = response.split("<start_of_turn>model")[-1].strip()

        print("üí¨ CHAT TEMPLATE RESPONSE:")
        print(model_part)

        return self._parse_gemma_response(model_part, text)

    def _parse_gemma_response(self, response, original_text):
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç Gemma"""
        try:
            # –ò—â–µ–º JSON
            json_match = re.search(r'\{[^}]+\}', response)
            if json_match:
                data = json.loads(json_match.group())

                # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                if "–ò–ù–ù" in data and data["–ò–ù–ù"]:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
                    inn_clean = re.sub(r'\D', '', str(data["–ò–ù–ù"]))
                    if len(inn_clean) in [10, 12]:
                        data["–ò–ù–ù"] = inn_clean
                    else:
                        data["–ò–ù–ù"] = None

                if "–§–ò–û" in data and data["–§–ò–û"]:
                    # –û—á–∏—â–∞–µ–º –§–ò–û
                    fio_clean = re.sub(r'[^–∞-—è–ê-–Ø—ë–Å\s]', '', str(data["–§–ò–û"])).strip()
                    data["–§–ò–û"] = fio_clean if fio_clean else None

                return data
            else:
                return {
                    "error": "JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ",
                    "raw_response": response,
                    "original_text": original_text
                }

        except json.JSONDecodeError as e:
            return {
                "error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}",
                "raw_response": response,
                "original_text": original_text
            }


def test_gemma():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º Gemma"""

    extractor = GemmaExtractor()

    test_cases = [
        "–ê–∫–∫—Ä n 123, –ò–Ω–Ω 4353229845, –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á",
        "–ö–ª–∏–µ–Ω—Ç: –ü–µ—Ç—Ä–æ–≤ –ê–ª–µ–∫—Å–µ–π –°–µ—Ä–≥–µ–µ–≤–∏—á, –ò–ù–ù 123456789012",
        "–§–ò–û: –°–∏–¥–æ—Ä–æ–≤–∞ –ú–∞—Ä–∏—è, –∏–Ω–Ω 9876543210",
        "–ü—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç –±–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        "–ò–ù–ù 1111111111 –¥–ª—è John Doe"
    ]

    print("üß™ –¢–ï–°–¢–ò–†–£–ï–ú GEMMA-2-2b-it")
    print("=" * 60)

    for i, text in enumerate(test_cases, 1):
        print(f"\nüéØ –¢–ï–°–¢ {i}: {text}")

        # –ü—Ä–æ–±—É–µ–º –æ–±–∞ –º–µ—Ç–æ–¥–∞
        print("\n1. üìù –ü—Ä—è–º–æ–π –ø—Ä–æ–º–ø—Ç:")
        result1 = extractor.extract_inn_and_name(text)
        print(json.dumps(result1, ensure_ascii=False, indent=2))

        print("\n2. üí¨ –ß–∞—Ç-—Ç–µ–º–ø–ª–µ–π—Ç:")
        result2 = extractor.extract_with_chat_template(text)
        print(json.dumps(result2, ensure_ascii=False, indent=2))

        print("\n" + "‚îÄ" * 50)


if __name__ == "__main__":
    test_gemma()