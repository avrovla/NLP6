import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path


class CopiedGemmaChat:
    def __init__(self):
        # –ü—É—Ç—å –∫ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∫–∞—Ç–∞–ª–æ–≥—É
        self.model_path = Path("models") / "models--google--gemma-2-2b-it"

        print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Gemma –∏–∑ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞...")
        print(f"üìÅ –ü—É—Ç—å: {self.model_path}")

        if not self.model_path.exists():
            print("‚ùå –ö–∞—Ç–∞–ª–æ–≥ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            self.ready = False
            return

        # –ü—Ä–æ–≤–µ—Ä–∏–º —á—Ç–æ –≤–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å snapshots
        snapshots_path = self.model_path / "snapshots"
        if not snapshots_path.exists():
            print("‚ùå –ü–∞–ø–∫–∞ snapshots –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            self.ready = False
            return

        # –ù–∞–π–¥–µ–º –ø–∞–ø–∫—É —Å —Ö–µ—à–µ–º (–ø–µ—Ä–≤–∞—è –≤ snapshots)
        snapshot_dirs = list(snapshots_path.iterdir())
        if not snapshot_dirs:
            print("‚ùå –ù–µ—Ç snapshot'–æ–≤ –≤ –ø–∞–ø–∫–µ")
            self.ready = False
            return

        # –ü—É—Ç—å –∫ —Ä–µ–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º –º–æ–¥–µ–ª–∏
        self.actual_model_path = snapshot_dirs[0]
        print(f"üìÅ –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –≤: {self.actual_model_path}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –ø–∞–ø–∫–∏ —Å —Ñ–∞–π–ª–∞–º–∏ –º–æ–¥–µ–ª–∏
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.actual_model_path))
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.actual_model_path),
                torch_dtype=torch.bfloat16,
                device_map="auto",
                local_files_only=True  # –¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
            )

            print("‚úÖ Gemma –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞!")
            self.ready = True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            self.ready = False

    def chat(self, message: str) -> str:
        if not self.ready:
            return "‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."

        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
            prompt = f"–í–æ–ø—Ä–æ—Å: {message}\n–û—Ç–≤–µ—Ç:"

            inputs = self.tokenizer(prompt, return_tensors="pt")

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            bot_response = full_response.replace(prompt, "").strip()

            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            if '\n' in bot_response:
                bot_response = bot_response.split('\n')[0]

            return bot_response if bot_response else "–ù–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å"

        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"


def main():
    bot = CopiedGemmaChat()

    if not bot.ready:
        return

    print("\n" + "=" * 50)
    print("ü§ñ Gemma –∏–∑ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞")
    print("=" * 50)

    while True:
        try:
            user_input = input("\nüë§ –í—ã: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break

            print("ü§ñ Gemma: ", end="", flush=True)
            response = bot.chat(user_input)
            print(response)

        except KeyboardInterrupt:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break


if __name__ == "__main__":
    main()